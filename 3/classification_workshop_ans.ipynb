{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is one of the most frequent applications of machine learning. In this workshop, your two goals are to work through the entire machine learning pipeline (though the data is preprocessed) and compare different supervised methods. There will be little driver code for this workshop, as comfort with implementing the steps of the machine learning pipeline is crucial within all domains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries - look at the last workshop if you don't remember the 3 most essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the file \"spambase.csv\" as a dataframe, and display the first 5 rows to examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the spambase dataset is an email. The features are obtained by taking the frequency (which is normalized on email length) of certain words. Intuitively, emails containing words like \"free\" are likely to be spam emails, and using this feature representation we capture some of the important elements of the email's content. The label for the dataset is the binary \"is_spam\" column, which is 1 for spam emails and 0 for real emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataframe into features and label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the (features, labels) set into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit 3 different ML models to see which one does the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import logistic regression function from sklearn and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model on training set and predict on test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate classification models, 2 common methods are zero-one-loss and confusion matrix. Zero-one-loss is the most basic and intuitive way to evaluate classification - this error is just the fraction of incorrectly predicted labels, so a zero-one-loss of 0 is optimal.\n",
    "\n",
    "One issue with zero-one-loss is that it doesn't distinguish between false positives and false negatives. The confusion matrix provides more information by showing how many times each label was predicted for elements of each true label. Documentation is given on the sklearn reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display zero-one-loss and confusion matrix (importing from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16333622936576886"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import SVM function from sklearn and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model on training set and predict on test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display confusion matrix and zero-one-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import decision tree function from sklearn and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model on training set and predict on test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display confusion matrix and zero-one-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen how some of the most common vanilla classification methods have performed on the data. However, there are countless ways to build upon these foundational methods to create more powerful models. Your challenge is to read up on some advanced classification methods to get the highest possible predictive accuracy on the test set. \n",
    "\n",
    "Some methods that build on top of decision trees and SVMs are different kernel functions for SVMs, gradient boosted trees, and random forests. Other classification methods that we haven't discussed are neural networks and Bayesian models. All of these methods are provided in Scikit-learn and documented [here](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Read up on and try out the models that seem interesting or suitable for this problem, and try to get the highest accuracy out of all the teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat steps outlined above on different models to get the highest possible accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Really That Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you have seen some of the most commonly used machine learning techniques for regression and classification. You've also applied them as part of the machine learning pipeline to fit models, make predictions, and evaluate results. Evaluation of accuracy is crucial, as it allows you to determine which model is best for a given problem.\n",
    "\n",
    "After you have a fitted model, you can use it to predict on new data, where you have the features and want to know the labels. For example, now that we've fitted a model that takes in features (word frequencies) and outputs whether it believes the email to be spam, we can feed in new emails (after preprocessing the emails to create the word frequency features) and get predictions on whether these unknown emails are spam.  \n",
    "\n",
    "The big takeaway is that applying machine learning methods can really be this easy. Knowing the theory behind each model helps you understand which models work in which situations, and the implementation for most basic machine learning methods are provided in sklearn (or other libraries in different languages). To apply these methods and draw interesting conclusions from data, all you need to do is recreate the machine learning pipeline like we did today.\n",
    "\n",
    "Of course, there are many techniques such as feature extraction, ensembling, and k-fold cross validation for parameter tuning that we will not have time to get to in these workshops. These methods are very important, and any machine learning book will go much further in depth concerning advanced techniques. As for the workshops, we will be moving on from supervised learning to unsupervised learning, time series analysis, and deep learning in future lectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
